#!/usr/bin/env python
# coding: utf-8

import pandas as pd
import numpy as np
import pickle
import joblib
from interpret.glassbox import APLRClassifier
from interpret import show, set_visualize_provider
from interpret.provider import InlineProvider
from alibi.explainers import AnchorTabular, CounterfactualProto
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import os
import warnings

warnings.filterwarnings('ignore')

# TensorFlowé…ç½®
os.environ["TF_USE_LEGACY_KERAS"] = "1"
import tensorflow as tf

tf.get_logger().setLevel(40)
tf.compat.v1.disable_v2_behavior()

# é…ç½®
set_visualize_provider(InlineProvider())
SEED = 42
np.random.seed(SEED)

# æ•°æ®è·¯å¾„é…ç½®
DATA_PATH = "data/L3-4è¿‘ç«¯å’ŒL5-S1è¿œç«¯åˆå¹¶ç‰ˆ.xlsx"
L34_SHEET = "L34 Result"
L5S1_SHEET = "L5S1 Result"


class APLRPredictor:
    """APLRæ¨¡å‹é¢„æµ‹å™¨ç±»ï¼Œå¯ä»¥è¢«pickleåºåˆ—åŒ–"""

    def __init__(self, model, feature_names):
        self.model = model
        self.feature_names = feature_names

    def __call__(self, x):
        if isinstance(x, np.ndarray):
            x_df = pd.DataFrame(x, columns=self.feature_names)
            predictions = self.model.predict(x_df)
            return np.array([int(p) for p in predictions])
        return np.array([0])


class RFPredictor:
    """éšæœºæ£®æ—é¢„æµ‹å™¨ç±»ï¼Œå¯ä»¥è¢«pickleåºåˆ—åŒ–"""

    def __init__(self, model, scaler):
        self.model = model
        self.scaler = scaler

    def __call__(self, x):
        if isinstance(x, np.ndarray):
            if len(x.shape) == 1:
                x = x.reshape(1, -1)
            pred_labels = self.model.predict(self.scaler.transform(x))
            pred_proba = np.zeros((len(pred_labels), 2))
            for i, label in enumerate(pred_labels):
                pred_proba[i, int(label)] = 1.0
            return pred_proba
        return np.array([[1.0, 0.0]])


class EnhancedModelSaver:
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ‰€æœ‰è§£é‡Šç»„ä»¶"""

    def __init__(self):
        self.interventional_features = ['Cage height', 'Operative time ', 'Blood loss']
        self.feature_constraints = {
            'Cage height': (8.0, 16.0),
            'Operative time ': (0.0, 600.0),
            'Blood loss': (0.0, 1000.0)
        }

    def identify_categorical_features(self, X):
        """è¯†åˆ«åˆ†ç±»ç‰¹å¾"""
        categorical_features = {}
        categorical_indices = []

        for i, col in enumerate(X.columns):
            if X[col].dtype in ['int64', 'int32', 'object'] or X[col].nunique() <= 10:
                unique_vals = sorted(X[col].unique())
                categorical_features[i] = [str(val) for val in unique_vals]
                categorical_indices.append(i)

        return categorical_features, categorical_indices

    def build_autoencoder(self, input_dim, latent_dim=16):
        """æ„å»ºè‡ªç¼–ç å™¨"""
        input_layer = tf.keras.layers.Input(shape=(input_dim,))

        # ç¼–ç å™¨
        encoded = tf.keras.layers.Dense(32, activation='relu')(input_layer)
        encoded = tf.keras.layers.Dense(latent_dim, activation='relu')(encoded)

        # è§£ç å™¨
        decoded = tf.keras.layers.Dense(32, activation='relu')(encoded)
        decoded = tf.keras.layers.Dense(input_dim, activation='linear')(decoded)

        # æ„å»ºæ¨¡å‹
        autoencoder = tf.keras.models.Model(input_layer, decoded)
        encoder = tf.keras.models.Model(input_layer, encoded)

        # ç¼–è¯‘æ¨¡å‹
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)
        autoencoder.compile(optimizer=optimizer, loss='mse')

        return autoencoder, encoder

    def train_and_save_all(self):
        """è®­ç»ƒå¹¶ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œè§£é‡Šç»„ä»¶"""
        print("å¼€å§‹è®­ç»ƒå¹¶ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œè§£é‡Šç»„ä»¶...")

        # ç¡®ä¿modelsæ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs("models", exist_ok=True)

        for sheet_name, region in [(L34_SHEET, "L34"), (L5S1_SHEET, "L5S1")]:
            print(f"\nå¤„ç†{region}æ•°æ®é›†...")

            # åŠ è½½æ•°æ®
            data = pd.read_excel(DATA_PATH, sheet_name=sheet_name)
            X = data.drop('Result', axis=1)
            y = data['Result'].astype(int)
            feature_names = X.columns.tolist()

            print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
            print(f"ASDåˆ†å¸ƒ: {y.value_counts().to_dict()}")

            # 1. è®­ç»ƒAPLRæ¨¡å‹
            print("è®­ç»ƒAPLRæ¨¡å‹...")
            aplr_model = APLRClassifier(random_state=SEED)
            aplr_model.fit(X, y, X_names=feature_names)

            train_pred = np.array([int(p) for p in aplr_model.predict(X)])
            aplr_accuracy = np.mean(train_pred == y)
            print(f"APLRå‡†ç¡®ç‡: {aplr_accuracy:.4f}")

            # 2. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆç”¨äºåäº‹å®åˆ†æï¼‰
            print("è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)

            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            rf_model = RandomForestClassifier(n_estimators=100, random_state=SEED)
            rf_model.fit(X_train_scaled, y_train)

            rf_pred = rf_model.predict(X_test_scaled)
            rf_accuracy = np.mean(rf_pred == y_test)
            print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {rf_accuracy:.4f}")

            # 3. è®­ç»ƒè‡ªç¼–ç å™¨
            print("è®­ç»ƒè‡ªç¼–ç å™¨...")
            autoencoder, encoder = self.build_autoencoder(X_train_scaled.shape[1], latent_dim=16)
            autoencoder.fit(
                X_train_scaled, X_train_scaled,
                epochs=50, batch_size=24,  # å‡å°‘epochsé¿å…è¿‡é•¿è®­ç»ƒæ—¶é—´
                validation_split=0.2, verbose=0
            )

            # 4. è®¾ç½®é”šç‚¹è§£é‡Šå™¨
            print("è®¾ç½®é”šç‚¹è§£é‡Šå™¨...")
            categorical_features, _ = self.identify_categorical_features(X)

            # åˆ›å»ºå¯åºåˆ—åŒ–çš„é¢„æµ‹å™¨
            aplr_predictor = APLRPredictor(aplr_model, feature_names)

            anchor_explainer = AnchorTabular(
                predictor=aplr_predictor,
                feature_names=feature_names,
                categorical_names=categorical_features if categorical_features else None
            )

            anchor_explainer.fit(X.values, disc_perc=[25, 50, 75])

            # 5. è®¾ç½®åäº‹å®è§£é‡Šå™¨
            print("è®¾ç½®åäº‹å®è§£é‡Šå™¨...")
            rf_predictor = RFPredictor(rf_model, scaler)

            cf_explainer = CounterfactualProto(
                rf_predictor,
                shape=(1, X_train.shape[1]),
                beta=0.1,
                gamma=100.0,
                theta=100.0,
                ae_model=autoencoder,
                enc_model=encoder,
                max_iterations=200,  # å‡å°‘è¿­ä»£æ¬¡æ•°
                feature_range=(np.min(X_train.values, axis=0), np.max(X_train.values, axis=0)),
                c_init=1.0,
                c_steps=3,
                learning_rate_init=0.01,
                clip=(-1000.0, 1000.0)
            )

            print("æ‹Ÿåˆåäº‹å®è§£é‡Šå™¨...")
            cf_explainer.fit(X_train_scaled)

            # 6. ç”Ÿæˆå…¨å±€å’Œå±€éƒ¨è§£é‡Š
            print("ç”Ÿæˆè§£é‡Šç»“æœ...")
            global_exp = aplr_model.explain_global(name=f"{region} å…¨å±€è§£é‡Š")

            # é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬ç”Ÿæˆå±€éƒ¨è§£é‡Š
            high_risk_indices = np.where(y == 1)[0][:3]
            low_risk_indices = np.where(y == 0)[0][:3]
            selected_indices = np.concatenate([high_risk_indices, low_risk_indices])

            local_explanations = []
            for idx in selected_indices:
                sample_X = X.iloc[[idx]]
                sample_y = y.iloc[idx]

                local_exp = aplr_model.explain_local(
                    sample_X.values,
                    np.array([sample_y]),
                    name=f"{region} æ ·æœ¬{idx} å±€éƒ¨è§£é‡Š"
                )

                local_explanations.append({
                    'index': idx,
                    'explanation': local_exp,
                    'true_label': sample_y,
                    'features': sample_X.iloc[0].to_dict()
                })

            # 7. ä¿å­˜æ‰€æœ‰ç»„ä»¶
            print("ä¿å­˜æ‰€æœ‰ç»„ä»¶...")

            # ä¿å­˜åŸºç¡€æ¨¡å‹
            joblib.dump(aplr_model, f"models/{region}_aplr_model.joblib")
            joblib.dump(rf_model, f"models/{region}_rf_model.joblib")
            joblib.dump(scaler, f"models/{region}_scaler.joblib")

            # ä¿å­˜è‡ªç¼–ç å™¨ï¼ˆä½¿ç”¨TensorFlowçš„ä¿å­˜æ–¹å¼ï¼‰
            autoencoder.save_weights(f"models/{region}_autoencoder_weights.h5")
            encoder.save_weights(f"models/{region}_encoder_weights.h5")

            # ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
            autoencoder_config = {
                'input_dim': X_train_scaled.shape[1],
                'latent_dim': 16
            }
            with open(f"models/{region}_autoencoder_config.pkl", 'wb') as f:
                pickle.dump(autoencoder_config, f)

            # ä¿å­˜è§£é‡Šå™¨ç»„ä»¶ï¼ˆåˆ†åˆ«ä¿å­˜ï¼Œé¿å…pickleé—®é¢˜ï¼‰
            try:
                # ä¿å­˜é”šç‚¹è§£é‡Šå™¨çš„è®­ç»ƒæ•°æ®å’Œé…ç½®
                anchor_data = {
                    'categorical_features': categorical_features,
                    'feature_names': feature_names,
                    'training_data': X.values,
                    'disc_perc': [25, 50, 75]
                }
                with open(f"models/{region}_anchor_data.pkl", 'wb') as f:
                    pickle.dump(anchor_data, f)

                print(f"é”šç‚¹è§£é‡Šå™¨æ•°æ®å·²ä¿å­˜")
            except Exception as e:
                print(f"é”šç‚¹è§£é‡Šå™¨ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜è§£é‡Šç»“æœ
            with open(f"models/{region}_global_explanation.pkl", 'wb') as f:
                pickle.dump(global_exp, f)

            with open(f"models/{region}_local_explanations.pkl", 'wb') as f:
                pickle.dump(local_explanations, f)

            # ä¿å­˜æ•°æ®ä¿¡æ¯
            data_info = {
                'feature_names': feature_names,
                'data_shape': data.shape,
                'asd_distribution': y.value_counts().to_dict(),
                'aplr_accuracy': aplr_accuracy,
                'rf_accuracy': rf_accuracy,
                'interventional_features': self.interventional_features,
                'feature_constraints': self.feature_constraints
            }

            with open(f"models/{region}_data_info.pkl", 'wb') as f:
                pickle.dump(data_info, f)

            # ä¿å­˜è®­ç»ƒæ•°æ®
            training_data = {
                'X': X, 'y': y,
                'X_train': X_train, 'X_test': X_test,
                'y_train': y_train, 'y_test': y_test,
                'X_train_scaled': X_train_scaled,
                'X_test_scaled': X_test_scaled
            }
            with open(f"models/{region}_training_data.pkl", 'wb') as f:
                pickle.dump(training_data, f)

            print(f"{region} æ‰€æœ‰ç»„ä»¶ä¿å­˜å®Œæˆ")

        print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹å’Œè§£é‡Šç»„ä»¶ä¿å­˜å®Œæˆï¼")
        print("ä¿å­˜çš„æ–‡ä»¶ï¼š")
        for file in sorted(os.listdir("models")):
            if file.endswith(('.pkl', '.joblib', '.h5')):
                print(f"  - models/{file}")


if __name__ == "__main__":
    saver = EnhancedModelSaver()
    saver.train_and_save_all()